{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Math_of_Layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi0xI0B74/oxuWJaRfCXf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenxuan0923/My-notes/blob/master/DL_Math_of_Layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N-qpdi79YoL",
        "colab_type": "text"
      },
      "source": [
        "## Math under the hood: Layers\n",
        "\n",
        "Last time we build our first DL model using dummy data. This note will explain the following questions:\n",
        "* **What is heppening in `keras.layers.Dense`?**\n",
        "* **Why the number of params is 16?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fadtJx4B8uOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data = np.random.random((50, 3))\n",
        "target = 2*np.sum(data, axis=1) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfsaDYdI83q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "da7234e2-97b2-4896-b5d8-f3dbb578446a"
      },
      "source": [
        "import keras\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(units=3, input_dim=3))\n",
        "model.add(keras.layers.Dense(units=1))\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oiTyQ_U8hf_",
        "colab_type": "text"
      },
      "source": [
        "What these Layers do is, they extract representations out of the data fed into them through **data transformation**.\n",
        "\n",
        "> You can consider it as a **data distillation** process:\n",
        "> * Some data goes in and comes out in a more informative form <br>\n",
        "> * Some unuseful information will be dropped during this process\n",
        "\n",
        "**So thatâ€™s what machine learning is, technically:**\n",
        "- searching for useful representations (defined by parameters in layers) of some input data \n",
        "- within a predefined space of possibilities (defined by the structure of network) \n",
        "- using guidance from a feedback signal (defined by loss score and optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7mUDeHsARlP",
        "colab_type": "text"
      },
      "source": [
        "#### Math under the hood\n",
        "\n",
        "**The data transformation for a $\\color{red}{\\text{single}}$ data point:** \n",
        "\n",
        "$$output = \\sigma(W^T\\boldsymbol{x}+b)$$\n",
        "\n",
        "Where $\\boldsymbol{x} = [x_1, x_2, x_3]^T$ and $\\sigma$ is an activation function. \n",
        "\n",
        "To further unpack the formula:\n",
        "\n",
        "$$\\sigma(W^T\\boldsymbol{x}+b) =\\sigma \\Bigl([w_{1}, w_{2}, w_{3}]\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + b\\Bigr) = \\sigma(w_1x_1 + w_2x_2 + w_3x_3 + b) = \\text{some scalar}$$\n",
        "\n",
        "**To process the data for the $\\color{red}{\\text{whole dataset}}$ with $m$ samples:**\n",
        "\n",
        "We can simply stack the sample data vertically:\n",
        "\n",
        "$$\\sigma(W^TX+b) = \\sigma\\Bigl([w_{1}, w_{2}, w_{3}]\\begin{bmatrix} \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\ x^{(1)} & x^{(2)} & \\dots &x^{(m)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix} + b\\Bigr) = \\begin{bmatrix} \\sigma(W^Tx^{(1)}+b) \\\\ \\vdots\\ \\\\ \\sigma(W^Tx^{(m)}+b) \\end{bmatrix} = \\text{some vector} $$\n",
        "\n",
        "Where $x^{(i)}$ represents the i-th sample in the dataset.\n",
        "\n",
        "This is called **vectorization**, which can greatly speed up the algorithm.\n",
        "This whole process illustrated above is **a neural network with 1 hidden layer and 1 hidden unit**. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src = 'https://drive.google.com/uc?id=1JX74MDF6yMrxkH9HlVsigxVsmtK4U6a7'\n",
        "width=\"350\" height=\"240\" style=\"vertical-align:middle\"/>\n",
        "</p>\n",
        "\n",
        "It becomes the well known **Logistic Regression Model** when we choose softmax as activation function.\n",
        "\n",
        "With more hidden units or layers, we cam get a deep neural netword, let the one we defined in the example above.\n",
        "\n",
        "**The structure of a neural network with 1 hidden layer with 3 units:**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src = 'https://drive.google.com/uc?id=16Wd_swdEhLn4bdT1JgJ4BskJp_G_-s-1'\n",
        "width=\"380\" height=\"340\" style=\"vertical-align:middle\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe-z5LPo8vHE",
        "colab_type": "text"
      },
      "source": [
        "Here we use square bracket `[]` to represent the number of layer, and subscript to represent the hidden unit. <br>\n",
        "For example: $W^{[1]}_{1}$ represent the first hidden unit in the first layer.\n",
        "\n",
        "#### The data transformation for this network is:\n",
        "\n",
        "**Hidden Layer**:\n",
        "\n",
        "$$h_1 = \\sigma(W^{[1]}_{1} X + b^{[1]}_{1})$$\n",
        "$$h_2 = \\sigma(W^{[1]}_{2} X + b^{[1]}_{2})$$\n",
        "$$h_3 = \\sigma(W^{[1]}_{3} X + b^{[1]}_{3})$$\n",
        "\n",
        "We can further vectorize this expression:\n",
        "\n",
        "$$h^{[1]} = \\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{bmatrix} = \\begin{bmatrix} \\sigma(W_{1}^{[1]}X + b_{1}^{[1]}) \\\\  \\sigma(W_{2}^{[1]}X + b_{2}^{[1]}) \\\\ \\sigma(W_{3}^{[1]}X + b_{3}^{[1]}) \\end{bmatrix} = \\sigma \\Biggl( \\begin{bmatrix} \\dots & W_{1}^{[1]} & \\dots \\\\ \\dots & W_{2}^{[1]} & \\dots \\\\ \\dots & W_{3}^{[1]} & \\dots \\end{bmatrix} X+ \\begin{bmatrix} b_{1}^{[1]} \\\\  b_{2}^{[1]} \\\\ b_{3}^{[1]} \\end{bmatrix}\\Biggr) = \\sigma(W^{[1]}X + b^{[1]})$$\n",
        "\n",
        "**Output Layer**:\n",
        "\n",
        "$$y' = W^{[2]}h^{[1]} + b^{[2]}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu-gFAxxCTci",
        "colab_type": "text"
      },
      "source": [
        "**Number of parameters in this network**\n",
        "\n",
        "* $W^{[1]}$ has three vecors $W^{[1]}_{1}$, $W^{[1]}_{2}$, $W^{[1]}_{3}$, each of them has 3 dimensions $[w_1, w_2, w_3]$.\n",
        "* $b^{[1]}$ has three scalar $b^{[1]}_{1}$, $b^{[1]}_{2}$, $b^{[1]}_{3}$\n",
        "* $W^{[2]}$ has one 3D vecor\n",
        "* $b^{[2]}$ has one scalar\n",
        "\n",
        "The total number of parameters = 3*3 + 3 + 3 + 1 = 16\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sn9jSiFCT1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}