{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/ Exploding Gradients\n",
    "\n",
    "This note will answer the following question:\n",
    "- Cause of gradient vanishing/exploding problem\n",
    "> - Bad initial weights $W$\n",
    "> - Bad choice of activation function\n",
    "- What is overflow and underflow\n",
    "\n",
    "\n",
    "### 1. Bad Initial Weights\n",
    "\n",
    "Consider a deep neural network $l$ hidden layers:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1AvMQI_iYsfr4tqSsxGJRYw4lKe7CToQq' width=800></img></center>\n",
    "\n",
    "with linear activation function: $g(z) = z$ parameters $b^{[i]} = 0$ for all layers. Then the output of this NN will be: \n",
    "\n",
    "$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]}...W^{[3]}W^{[2]}W^{[1]}x$$\n",
    "\n",
    "This is because:\n",
    "\n",
    "$$a^{[1]}=g(z^{[1]})=g(W^{[1]}x+b^{[1]})=W^{[1]}x$$\n",
    "\n",
    "$$a^{[2]}=g(z^{[2]})=g(W^{[2]}a^{[1]}+b^{[2]})=W^{[2]}a^{[1]}=W^{[2]}W^{[1]}x$$\n",
    "\n",
    "$$a^{[3]}=g(z^{[3]})=g(W^{[3]}a^{[2]}+b^{[3]})=W^{[3]}a^{[2]}=W^{[3]}W^{[2]}W^{[1]}x$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "Since each intermediate hidden layer has 2 hidden units, and input $x=[x_1, x_2]$ also have two dimension, $W^{[i]}$ will be 2\\*2 matrix (**Note**: except the last layer $W^{[l]}$).\n",
    "\n",
    "Now imagin all the $W^{[i]}$ have the same value:\n",
    "\n",
    "$$ W^{[i]} = \\begin{bmatrix}\n",
    "1.5 & 0 \\\\\n",
    "0 & 1.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then the output value:\n",
    "\n",
    "$$\\hat{y} = W^{[l]}\\begin{bmatrix}\n",
    "1.5 & 0 \\\\\n",
    "0 & 1.5\n",
    "\\end{bmatrix}^{l-1} x =  W^{[l]}\\begin{bmatrix}\n",
    "1.5^{l-1} & 0 \\\\\n",
    "0 & 1.5^{l-1}\n",
    "\\end{bmatrix} x$$\n",
    "\n",
    "When the deep neural is very deep (with very large $l$), the output $\\hat{y}$ will be super large: $1.5^{l-1} \\rightarrow \\infty$ as $l \\rightarrow \\infty$. \n",
    "\n",
    "This can result in very large gradients during back propagation. The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than 1.0. \n",
    "\n",
    "In the most extreme case, the values of parameters can become so large as to overflow and result in NaN values.\n",
    "\n",
    "> **Note**: A rounding error occurs when numbers with very large magnitude being approximated as $\\pm \\infty$, further arithmetic will change these values into NaN on a computer, this is called **overflow**. When\n",
    "> - The model is unstable, resulting in large changes in loss through iterations\n",
    "> - The model loss goes to NaN during training\n",
    ">\n",
    "> these are signs of **gradient exploding problem**\n",
    "\n",
    "Similarly, if all the $W^{[i]}$ have value:\n",
    "\n",
    "$$ W^{[i]} = \\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 0.5\n",
    "\\end{bmatrix}$$<br>\n",
    "\n",
    "Then the output value:\n",
    "\n",
    "$$\\hat{y} = W^{[l]}\\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 0.5\n",
    "\\end{bmatrix}^{l-1} x =  W^{[l]}\\begin{bmatrix}\n",
    "0.5^{l-1} & 0 \\\\\n",
    "0 & 0.5^{l-1}\n",
    "\\end{bmatrix} x$$\n",
    "<br>\n",
    "\n",
    "When the neural network is very deep (with very large $l$), the output $\\hat{y}$ will be super small: $0.5^{l-1} \\rightarrow 0$ as $l \\rightarrow \\infty$. This is the case for all $W^{[i]} < I \\text{  (Identity matrix)} $\n",
    "\n",
    "> **Note**: A rounding error occurs when numbers near zero are rounded to zero during calculation on a computer, this is called **underflow**. When\n",
    ">\n",
    "> - The training loss remain large and nearly unchanged.\n",
    "> It's a sign of **gradient vanishing problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bad choice of activation function\n",
    "\n",
    "Recall that is the back propagation, we need to take the derivative of the activation function w.r.t all the $a^{[i]}$, if sigmoid function is being used as the activation function:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1v035gDUHLW-dck7ji82JChki7m30zhbj'></img></center>\n",
    "\n",
    "<br>\n",
    "The gradient is almost 0 when the value has a very large magnitude. Thus, during the backpropagation process, there is almost no updates of the parameters ($W^{[i]}$s and $b^{[i]}$s).\n",
    "\n",
    "This is a typical **gradient vanishing problem**. To fix it, use `relu` or other activation functions instead of sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example: **softmax function**\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{\\text{exp}(x_i)}{\\sum_{j=1}^{n}\\text{exp}(x_j)}$$\n",
    "\n",
    "Consider a simple case where all the $x_i = c$, we should expect to see all the outputs equal to $\\frac{1}{n}$. However,\n",
    "\n",
    "- When c is negative with large magnitude: then $exp(c) \\rightarrow 0$ will underflow, meaning the denominator of the softmax will become 0, so the final result will be undefined\n",
    "- When c is positive with large magnitude: then $exp(c) \\rightarrow +\\infty$ will overflow, the whole expression will become undefined\n",
    "\n",
    "This problem can be solved by evaluating $\\text{softmax}(z_i)$ instead, where $z_i = x_i-\\underset{0 \\leq j \\leq n}{\\max}(x_j)$. \n",
    "\n",
    "By doing so, the max value of $\\text{exp}(z_i)$ equals to  $\\text{exp}(0) = 1$ thus rules out the probability of overflow.\n",
    "At the same time, at least one term in the denominatore hae value 1, while rules out the possibility of underflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful link:\n",
    "\n",
    "[How to Fix the Vanishing Gradients Problem Using the ReLU](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/)\n",
    "\n",
    "[A Gentle Introduction to Exploding Gradients in Neural Networks](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
